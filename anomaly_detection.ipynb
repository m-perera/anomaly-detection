{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "import torch.nn as nn \n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Path to image folder\n",
    "current_working_dir = os.getcwd()\n",
    "image_folder = f\"{current_working_dir}/Manipulated\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforms.Compase([]) - function that chains multiple image transformations togetther \n",
    "# transforms.Resize(()) - resizes the input image to a fixed size \n",
    "# transforms.ToTensor() - this converst the image from PIL image/numpy array to a Pytorch Tensor\n",
    "\n",
    "#So, the entire \"transform\" object will first resize to 256x256 pixels then convert it to a tensor with pixel values scalled between 0 and 1\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256,256)),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gets the names of the subfolders\n",
    "\n",
    "# for f in os.listdir(image_folder) - iterates from the folder and assgins f to all the items inside \n",
    "# if os.path.isdir(os.path.join(image_folder, f)) - so this is a boolean statement, saying make a path by joining the mainfolder and each item in the folder, we check to see if that joined path is a directory and if it is we return \"f\" to the list variable (so we get the name of the directory)\n",
    "\n",
    "subfolders = [f for f in os.listdir(image_folder) if os.path.isdir(os.path.join(image_folder, f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1a8jd2', '1a84zh', '1af2jv', '1aeur2', '1a5h1p', '1a5x44', '1acw36', '1a9tss', '1a3oag', '1a69n6', '1aa8xl', '1a4zdz', '1a41rr', '1aeqsl', '1a6upj', '1aafqb', '1ad1a0', '1a16mu', '1a9l4s', '1ac1g7', '1a1ogs', '1aczjh', '1a07yi', '1aa6sn', '1a4dqp']\n"
     ]
    }
   ],
   "source": [
    "print(subfolders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_images = []\n",
    "manipulated_images = []\n",
    "original_labels = []\n",
    "manipulated_labels = []\n",
    "\n",
    "for subfolder in subfolders:\n",
    "    subfolder_path = os.path.join(image_folder, subfolder)\n",
    "\n",
    "    #get the original image\n",
    "    original_image_path = os.path.join(subfolder_path, f\"{subfolder}_orig.jpg\")\n",
    "\n",
    "    if os.path.exists(original_image_path):\n",
    "        original_image = Image.open(original_image_path)\n",
    "        original_image_tensor = transform(original_image)\n",
    "\n",
    "        original_images.append(original_image_tensor)\n",
    "        original_labels.append(0) #Label 0 for the original images\n",
    "    \n",
    "        #Now Deal with the Manipulated Images\n",
    "        manipulated_files = [f for f in os.listdir(subfolder_path) if f.endswith(\"_0.jpg\")]\n",
    "\n",
    "        #Processing the Manipulated Files\n",
    "        for manipulated_file in manipulated_files:\n",
    "            manipulated_file_path = os.path.join(subfolder_path, manipulated_file)\n",
    "\n",
    "            #Open the image from the path and transform using function\n",
    "            manipulated_image = Image.open(manipulated_file_path)\n",
    "            manipulated_image_tensor = transform(manipulated_image)\n",
    "\n",
    "            manipulated_images.append(manipulated_image_tensor)\n",
    "            manipulated_labels.append(1)\n",
    "    else:\n",
    "        print(f\"OG image not found in {subfolder}\")\n",
    "\n",
    "\n",
    "#Stack Tensors like a Matrix \n",
    "original_stacked = torch.stack(original_images)\n",
    "manipulated_stacked = torch.stack(manipulated_images)\n",
    "\n",
    "\n",
    "#Combine original and manipulated images into one tensor\n",
    "all_the_images = torch.cat((original_stacked, manipulated_stacked), dim=0)\n",
    "\n",
    "#Combine original and manipulated labels into one tensor\n",
    "all_the_labels = torch.tensor(original_labels + manipulated_labels)\n",
    "\n",
    "train_images, test_images, train_labels, test_labels = train_test_split(all_the_images,all_the_labels, test_size=0.2, random_state=23)\n",
    "\n",
    "#Make Tensorflow Dataset - creating pairs (original tensor, manipulated tensor)\n",
    "train_dataset = TensorDataset(train_images,train_labels)\n",
    "test_dataset = TensorDataset(test_images,test_labels)\n",
    "\n",
    "#Make DataLoader for batches and shuffling \n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "#Load pretrained Vgg16 model \n",
    "model = models.vgg16(weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the final layer to output 2 classes(manipulated and original)\n",
    "model.classifier[6] = nn.Linear(model.classifier[6].in_features,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loss Function and Optimizer \n",
    "\n",
    "#Loss Function \n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "#Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Device either to use GPU or CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#To use either GPU or CPU\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0\n",
      "Iteration: 1\n"
     ]
    }
   ],
   "source": [
    "#Training \n",
    "\n",
    "total_iterations = 2\n",
    "\n",
    "for iteration in range(total_iterations):\n",
    "    #set model to training mode\n",
    "    model.train() \n",
    "    \n",
    "    #loss across all the batches in each iteration\n",
    "    running_loss = 0.0\n",
    "    #number of correction predictions in each iteration\n",
    "    total_correct = 0\n",
    "    #number of samples processed in each iteration\n",
    "    total_processed = 0\n",
    "\n",
    "    #DataLoader Loop\n",
    "        # Go through dataloader, get the index of the current batch and the batch data(inputs, labels), and the training data in batches inputs(images) and labels(correct answer)\n",
    "    for inputs, labels in train_dataloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        #Model Training\n",
    "        optimizer.zero_grad()  # parameter gradients to zero\n",
    "        outputs = model(inputs) #forward pass\n",
    "        max_scores, predicted = torch.max(outputs, 1) #get the predictions\n",
    "        cal_loss = loss_function(outputs, labels) #calculate the loss\n",
    "        cal_loss.backward() #backward pass\n",
    "        optimizer.step() # optimize the weights\n",
    "\n",
    "        #Updating the running loss, and accuracy\n",
    "        running_loss += cal_loss.item()\n",
    "        total_correct += (predicted == labels).sum().item()\n",
    "        total_processed += labels.size(0)\n",
    "    \n",
    "    iteration_loss = running_loss / len(train_dataloader)\n",
    "    iteration_accuracy = total_correct / total_processed\n",
    "\n",
    "    print(f\"Iteration: {iteration}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8181818181818182\n"
     ]
    }
   ],
   "source": [
    "#Test\n",
    "\n",
    "model.eval()\n",
    "number_of_correct = 0\n",
    "total_test = 0\n",
    "\n",
    "with torch.no_grad(): #disable gradient computation while testing\n",
    "    for inputs, labels in test_dataloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        max_scores, predicted = torch.max(outputs, 1)\n",
    "        number_of_correct += (predicted == labels).sum().item()\n",
    "        total_test += labels.size(0)\n",
    "\n",
    "test_accuracy = number_of_correct/total_test\n",
    "print(test_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
